#!/bin/bash

# ğŸ¯ DEMONSTRAÃ‡ÃƒO: High Availability & Failover AutomÃ¡tico
# Objetivo: Mostrar como Citus mantÃ©m disponibilidade mesmo com falhas

set -uo pipefail

# Cores
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
PURPLE='\033[0;35m'
CYAN='\033[0;36m'
BOLD='\033[1m'
NC='\033[0m'

# ConfiguraÃ§Ãµes
COMPOSE_PROJECT_NAME="adtech_cluster"
DB_NAME="adtech_platform"

echo -e "${PURPLE}â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—${NC}"
echo -e "${PURPLE}â•‘              ğŸ›¡ï¸  HIGH AVAILABILITY & FAILOVER                â•‘${NC}"
echo -e "${PURPLE}â•‘      Demonstrando resiliÃªncia em ambiente distribuÃ­do       â•‘${NC}"
echo -e "${PURPLE}â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•${NC}"
echo

# Verificar se cluster estÃ¡ rodando
if ! docker exec "${COMPOSE_PROJECT_NAME}_master" pg_isready -U postgres > /dev/null 2>&1; then
    echo -e "${RED}âŒ Cluster bÃ¡sico nÃ£o estÃ¡ rodando!${NC}"
    echo -e "${YELLOW}ğŸ’¡ Execute ./02_simple_setup.sh primeiro${NC}"
    exit 1
fi

# FunÃ§Ã£o para mostrar topologia atual
show_topology() {
    local step="$1"
    echo -e "${CYAN}ğŸ—ï¸  $step - TOPOLOGIA ATUAL${NC}"
    echo -e "${CYAN}â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•${NC}"
    
    echo -e "${BOLD}ğŸ“Š COORDINATOR:${NC}"
    if docker exec "${COMPOSE_PROJECT_NAME}_master" pg_isready -U postgres > /dev/null 2>&1; then
        echo -e "${GREEN}âœ… Master: ${COMPOSE_PROJECT_NAME}_master (PRIMARY)${NC}"
    else
        echo -e "${RED}âŒ Master: ${COMPOSE_PROJECT_NAME}_master (DOWN)${NC}"
    fi
    
    echo -e "${BOLD}ğŸ”„ WORKERS:${NC}"
    docker exec -i "${COMPOSE_PROJECT_NAME}_master" psql -U postgres -d "$DB_NAME" -c "
    SELECT 
        nodename as worker,
        nodeport as port,
        CASE WHEN isactive THEN 'âœ… ACTIVE' ELSE 'âŒ INACTIVE' END as status
    FROM pg_dist_node 
    WHERE noderole = 'primary'
    ORDER BY nodename;
    " 2>/dev/null || echo "NÃ£o foi possÃ­vel conectar ao master"
    
    echo
}

# FunÃ§Ã£o para executar query de teste
test_query() {
    local description="$1"
    echo -e "${YELLOW}ğŸ” $description${NC}"
    
    start_time=$(date +%s.%N)
    if docker exec -i "${COMPOSE_PROJECT_NAME}_master" psql -U postgres -d "$DB_NAME" -c "
    SELECT 
        COUNT(*) as total_companies,
        SUM(CASE WHEN name LIKE 'Empresa Teste %' THEN 1 ELSE 0 END) as test_companies
    FROM companies;
    " 2>/dev/null; then
        end_time=$(date +%s.%N)
        duration=$(echo "$end_time - $start_time" | bc 2>/dev/null || echo "N/A")
        echo -e "${GREEN}âœ… Query executada com sucesso (${duration}s)${NC}"
    else
        echo -e "${RED}âŒ Query falhou - cluster pode estar indisponÃ­vel${NC}"
    fi
    echo
}

# FunÃ§Ã£o para simular falha
simulate_failure() {
    local target="$1"
    local description="$2"
    
    echo -e "${RED}ğŸ’¥ SIMULANDO FALHA: $description${NC}"
    echo -e "${RED}â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•${NC}"
    
    case $target in
        "worker1")
            echo -e "${YELLOW}ğŸ”Œ Desconectando Worker 1...${NC}"
            docker pause "${COMPOSE_PROJECT_NAME}_worker1" 2>/dev/null || true
            ;;
        "worker2")
            echo -e "${YELLOW}ğŸ”Œ Desconectando Worker 2...${NC}"
            docker pause "${COMPOSE_PROJECT_NAME}_worker2" 2>/dev/null || true
            ;;
        "master")
            echo -e "${YELLOW}ğŸ”Œ Desconectando Master...${NC}"
            docker pause "${COMPOSE_PROJECT_NAME}_master" 2>/dev/null || true
            ;;
    esac
    
    sleep 3
    echo -e "${RED}ğŸ’¥ Falha simulada!${NC}"
    echo
}

# FunÃ§Ã£o para recuperar serviÃ§o
recover_service() {
    local target="$1"
    local description="$2"
    
    echo -e "${GREEN}ğŸ”§ RECUPERANDO SERVIÃ‡O: $description${NC}"
    echo -e "${GREEN}â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•${NC}"
    
    case $target in
        "worker1")
            echo -e "${CYAN}ğŸ”„ Reconectando Worker 1...${NC}"
            docker unpause "${COMPOSE_PROJECT_NAME}_worker1" 2>/dev/null || true
            ;;
        "worker2")
            echo -e "${CYAN}ğŸ”„ Reconectando Worker 2...${NC}"
            docker unpause "${COMPOSE_PROJECT_NAME}_worker2" 2>/dev/null || true
            ;;
        "master")
            echo -e "${CYAN}ğŸ”„ Reconectando Master...${NC}"
            docker unpause "${COMPOSE_PROJECT_NAME}_master" 2>/dev/null || true
            ;;
    esac
    
    sleep 5
    echo -e "${GREEN}âœ… ServiÃ§o recuperado!${NC}"
    echo
}

echo -e "${BLUE}ğŸ¯ CENÃRIO INICIAL${NC}"
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"

show_topology "PASSO 1"
test_query "Query baseline (sistema saudÃ¡vel)"

echo -e "${CYAN}Pressione Enter para simular falha no Worker 1...${NC}"
read -r

echo -e "${BLUE}ğŸ’¥ CENÃRIO 1: FALHA DE WORKER${NC}"
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"

simulate_failure "worker1" "Worker 1 indisponÃ­vel"
show_topology "PASSO 2"

echo -e "${YELLOW}ğŸ¤” O que acontece com queries que dependem dos shards do Worker 1?${NC}"
test_query "Query durante falha de worker"

echo -e "${CYAN}Pressione Enter para recuperar Worker 1...${NC}"
read -r

recover_service "worker1" "Worker 1 voltando online"
show_topology "PASSO 3"
test_query "Query apÃ³s recuperaÃ§Ã£o do worker"

echo -e "${CYAN}Pressione Enter para simular falha mais severa...${NC}"
read -r

echo -e "${BLUE}ğŸ’¥ CENÃRIO 2: MÃšLTIPLAS FALHAS${NC}"
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"

simulate_failure "worker1" "Worker 1 down"
sleep 2
simulate_failure "worker2" "Worker 2 down (disaster!)"

show_topology "PASSO 4"

echo -e "${YELLOW}ğŸš¨ ATENÃ‡ÃƒO: Ambos workers falharam!${NC}"
echo "Isso representa um cenÃ¡rio de disaster recovery"
echo

test_query "Query durante disaster (esperado: falha total)"

echo -e "${CYAN}Pressione Enter para iniciar recuperaÃ§Ã£o de disaster...${NC}"
read -r

echo -e "${GREEN}ğŸ†˜ RECUPERAÃ‡ÃƒO DE DISASTER${NC}"
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"

recover_service "worker1" "Primeiro worker de volta"
test_query "Query com 1 worker recuperado (dados parciais)"

recover_service "worker2" "Segundo worker de volta"
show_topology "PASSO 5"
test_query "Query com cluster totalmente recuperado"

echo -e "${CYAN}Pressione Enter para demonstrar failover de coordinator...${NC}"
read -r

echo -e "${BLUE}ğŸ’¥ CENÃRIO 3: FALHA DO COORDINATOR${NC}"
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"

echo -e "${YELLOW}âš ï¸  Este Ã© o cenÃ¡rio mais crÃ­tico!${NC}"
echo "O coordinator Ã© o ponto central do cluster Citus"
echo

simulate_failure "master" "Coordinator indisponÃ­vel"

echo -e "${RED}ğŸš¨ SEM COORDINATOR = SEM CLUSTER${NC}"
echo "- NÃ£o Ã© possÃ­vel executar queries distribuÃ­das"
echo "- Workers ficam Ã³rfÃ£os"
echo "- AplicaÃ§Ãµes param de funcionar"
echo

test_query "Query sem coordinator (falha esperada)"

echo -e "${CYAN}Pressione Enter para recuperar coordinator...${NC}"
read -r

recover_service "master" "Coordinator voltando online"

# Aguardar coordinator se estabilizar
echo -e "${CYAN}â³ Aguardando coordinator se estabilizar...${NC}"
sleep 10

show_topology "PASSO 6"
test_query "Query apÃ³s recuperaÃ§Ã£o do coordinator"

echo -e "${BLUE}ğŸ“Š ANÃLISE DE RESILIÃŠNCIA${NC}"
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"

echo -e "${CYAN}ğŸ¯ LiÃ§Ãµes Aprendidas:${NC}"
echo
echo -e "${GREEN}âœ… CENÃRIOS QUE CITUS SUPORTA BEM:${NC}"
echo "â€¢ Falha de worker individual - queries continuam funcionando"
echo "â€¢ RecuperaÃ§Ã£o automÃ¡tica de workers"
echo "â€¢ RedistribuiÃ§Ã£o de carga entre workers saudÃ¡veis"
echo
echo -e "${YELLOW}âš ï¸  PONTOS DE ATENÃ‡ÃƒO:${NC}"
echo "â€¢ MÃºltiplas falhas simultÃ¢neas podem causar indisponibilidade"
echo "â€¢ Dados em workers falhados ficam temporariamente inacessÃ­veis"
echo "â€¢ Queries cross-shard sÃ£o mais vulnerÃ¡veis"
echo
echo -e "${RED}âŒ PONTOS CRÃTICOS:${NC}"
echo "â€¢ Falha do coordinator para todo o cluster"
echo "â€¢ Sem replicaÃ§Ã£o, dados podem ser perdidos"
echo "â€¢ Recovery manual pode ser necessÃ¡rio"

echo -e "${BLUE}ğŸ—ï¸  SOLUÃ‡Ã•ES PARA PRODUÃ‡ÃƒO${NC}"
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"

echo -e "${CYAN}ğŸ“‹ RecomendaÃ§Ãµes Arquiteturais:${NC}"
echo
echo "1. ğŸ”„ **COORDINATOR HA**"
echo "   â€¢ pg_auto_failover para coordinator"
echo "   â€¢ Streaming replication"
echo "   â€¢ VIP/Load balancer"
echo
echo "2. ğŸ­ **WORKER REPLICATION**"
echo "   â€¢ Cada worker com standby"
echo "   â€¢ ReplicaÃ§Ã£o sÃ­ncrona/assÃ­ncrona"
echo "   â€¢ Auto-failover por worker group"
echo
echo "3. ğŸ“Š **MONITORING**"
echo "   â€¢ Health checks contÃ­nuos"
echo "   â€¢ Alertas proativos"
echo "   â€¢ MÃ©tricas de replication lag"
echo
echo "4. ğŸ› ï¸  **OPERATIONAL**"
echo "   â€¢ Backups regulares"
echo "   â€¢ Disaster recovery testado"
echo "   â€¢ Runbooks documentados"

echo -e "${BLUE}ğŸ§ª IMPLEMENTAÃ‡ÃƒO HA AVANÃ‡ADA${NC}"
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"

echo -e "${YELLOW}ğŸ’¡ Para implementar HA real, seria necessÃ¡rio:${NC}"
echo

cat << 'EOF'
# docker-compose-ha.yml (exemplo conceitual)
services:
  # Monitor para pg_auto_failover
  monitor:
    image: citusdata/pg_auto_failover
    command: pg_autoctl create monitor
    
  # Coordinator Primary
  coordinator-primary:
    image: citusdata/citus:12.1
    command: pg_autoctl create coordinator
    
  # Coordinator Standby  
  coordinator-standby:
    image: citusdata/citus:12.1
    command: pg_autoctl create coordinator --monitor monitor:5432
    
  # Worker Group 1
  worker-1a:
    image: citusdata/citus:12.1
    command: pg_autoctl create worker --group 1
    
  worker-1b:
    image: citusdata/citus:12.1
    command: pg_autoctl create worker --group 1 --monitor monitor:5432
EOF

echo
echo -e "${CYAN}ğŸ”— Recursos Adicionais:${NC}"
echo "â€¢ pg_auto_failover docs: https://pg-auto-failover.readthedocs.io/"
echo "â€¢ Citus HA Guide: https://docs.citusdata.com/en/stable/admin_guide/high_availability.html"
echo "â€¢ Production Checklist: https://docs.citusdata.com/en/stable/admin_guide/cluster_management.html"

echo -e "${GREEN}â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—${NC}"
echo -e "${GREEN}â•‘                 ğŸ† DEMONSTRAÃ‡ÃƒO HA COMPLETA                 â•‘${NC}"
echo -e "${GREEN}â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•${NC}"
echo
echo -e "${CYAN}ğŸ¯ O que os alunos aprenderam:${NC}"
echo "   âœ… Como diferentes tipos de falha afetam o cluster"
echo "   âœ… ImportÃ¢ncia do coordinator para disponibilidade"
echo "   âœ… EstratÃ©gias de recuperaÃ§Ã£o manual"
echo "   âœ… Necessidade de arquitetura HA em produÃ§Ã£o"
echo "   âœ… Ferramentas como pg_auto_failover"
echo
echo -e "${PURPLE}ğŸ“š PrÃ³ximo: Explore arquiteturas HA reais em produÃ§Ã£o!${NC}"
echo -e "${PURPLE}Considere implementar pg_auto_failover em ambiente de teste${NC}"
